#!/usr/bin/env python3
"""
Results Parser for creport.txt

Parses the consolidated report (creport.txt) generated by the sorting stage
and converts it into structured JSON for API consumption and visualization.

Format of creport.txt:
- Lines 1-6: Supply condition metadata
- Line 7+: Header and data rows (tab-separated)

Example:
    supply condition
    v1: vcc
    v2: vccn
    v3: NA
    vcc_vid: vcc_ff_h#0.675,0.75,0.8424; ...
    
    process extract temp  v1   v2   fmax      trise     tfall  ...
    TT      typical 85    nom  nom  1.717e3   1.668e3   1.816e3  ...
    TT      typical 100   nom  nom  1.575e3   1.500e3   1.700e3  ...
"""

import os
import re


def parse_creport(creport_path):
    """
    Parse creport.txt into structured JSON.
    
    Args:
        creport_path (str): Path to creport.txt file
        
    Returns:
        dict: Parsed results with structure:
            {
                'metadata': {...},          # Supply conditions
                'headers': [...],           # Column names
                'rows': [{...}, ...],       # Data rows as dicts
                'total_corners': int        # Number of corners
            }
        None if file not found or parsing fails
    """
    if not os.path.exists(creport_path):
        return None
    
    try:
        with open(creport_path, 'r') as f:
            lines = f.readlines()
        
        if not lines:
            return None
        
        # Parse metadata (supply conditions)
        metadata = {}
        data_start_idx = 0
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # Check if this is the header line (starts with "process")
            if line.startswith('process'):
                data_start_idx = i
                break
            
            # Parse metadata lines
            if ':' in line:
                parts = line.split(':', 1)
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    metadata[key] = value
        
        if data_start_idx == 0:
            # No header found
            return None
        
        # Parse header
        header_line = lines[data_start_idx].strip()
        headers = [h.strip() for h in header_line.split('\t')]
        
        # Parse data rows
        results = []
        for i in range(data_start_idx + 1, len(lines)):
            line = lines[i].strip()
            
            if not line:
                continue  # Skip empty lines
            
            values = line.split('\t')
            
            if len(values) < len(headers):
                # Incomplete row, skip
                continue
            
            row = {}
            for j, header in enumerate(headers):
                if j < len(values):
                    value = values[j].strip()
                    
                    # Try to convert to float (for numeric columns)
                    try:
                        # Handle scientific notation (e.g., "1.717e3")
                        row[header] = float(value)
                    except ValueError:
                        # Keep as string (e.g., "TT", "typical", "nom")
                        row[header] = value
            
            results.append(row)
        
        return {
            'metadata': metadata,
            'headers': headers,
            'rows': results,
            'total_corners': len(results)
        }
    
    except Exception as e:
        print("Error parsing creport.txt: {0}".format(e))
        return None


def analyze_results(parsed_data, spec_limits=None):
    """
    Analyze parsed results and classify pass/fail.
    
    Args:
        parsed_data (dict): Parsed creport data from parse_creport()
        spec_limits (dict): Optional specification limits
            Example: {'rwkpull_vih': {'min': 1500, 'max': 2500}}
            If None, uses default limits
    
    Returns:
        dict: Analysis results with pass/fail classifications
    """
    if not parsed_data or not parsed_data.get('rows'):
        return None
    
    # Default spec limits for WKP characterization
    # These are typical values - users can override via API
    if spec_limits is None:
        spec_limits = get_default_spec_limits()
    
    analysis = {
        'total_corners': parsed_data['total_corners'],
        'pass_count': 0,
        'fail_count': 0,
        'warnings': [],
        'rows_with_status': [],
        'spec_limits_used': spec_limits  # Include which limits were applied
    }
    
    for row in parsed_data['rows']:
        row_status = 'pass'
        failures = []
        
        # Check each spec limit
        for param, limits in spec_limits.items():
            if param in row:
                value = row[param]
                
                # Skip non-numeric values (e.g., "error" from PrimeSim measurement failures)
                if not isinstance(value, (int, float)):
                    continue
                
                # Check min limit
                if 'min' in limits and value < limits['min']:
                    row_status = 'fail'
                    failures.append("{0} < {1} (min: {2})".format(
                        param, value, limits['min']))
                
                # Check max limit
                if 'max' in limits and value > limits['max']:
                    row_status = 'fail'
                    failures.append("{0} > {1} (max: {2})".format(
                        param, value, limits['max']))
        
        # Add status to row
        row_with_status = row.copy()
        row_with_status['_status'] = row_status
        row_with_status['_failures'] = failures if failures else None
        
        analysis['rows_with_status'].append(row_with_status)
        
        if row_status == 'pass':
            analysis['pass_count'] += 1
        else:
            analysis['fail_count'] += 1
    
    return analysis


def get_default_spec_limits():
    """
    Get default specification limits for WKP characterization.
    
    Returns:
        dict: Default spec limits
    """
    return {
        'rwkpull_vih': {'min': 1500, 'max': 2500},   # Weak pull-up resistance (Ohms)
        'rwkpull_vih2': {'min': 1500, 'max': 2500},  # Alternative measurement
        'rwkpull0': {'min': 1500, 'max': 2500},      # Initial resistance
        # Add more default specs as needed
        # 'ioh': {'min': -500e-6, 'max': -100e-6},   # Example: Output high current (A)
    }


def save_custom_spec_limits(spec_limits, config_file='spec_limits.json'):
    """
    Save custom specification limits to JSON file.
    
    Args:
        spec_limits (dict): Custom spec limits
        config_file (str): Path to config file
        
    Returns:
        bool: True if successful
    """
    try:
        import json
        with open(config_file, 'w') as f:
            json.dump(spec_limits, f, indent=2)
        return True
    except Exception as e:
        print("Error saving spec limits: {0}".format(e))
        return False


def load_custom_spec_limits(config_file='spec_limits.json'):
    """
    Load custom specification limits from JSON file.
    
    Args:
        config_file (str): Path to config file
        
    Returns:
        dict: Custom spec limits, or None if file doesn't exist
    """
    try:
        import json
        import os
        if os.path.exists(config_file):
            with open(config_file, 'r') as f:
                return json.load(f)
        return None
    except Exception as e:
        print("Error loading spec limits: {0}".format(e))
        return None


def export_to_csv(parsed_data, output_path):
    """
    Export parsed results to CSV format.
    
    Args:
        parsed_data (dict): Parsed creport data
        output_path (str): Path to output CSV file
        
    Returns:
        bool: True if successful, False otherwise
    """
    if not parsed_data or not parsed_data.get('rows'):
        return False
    
    try:
        import csv
        
        with open(output_path, 'w', newline='') as csvfile:
            headers = parsed_data['headers']
            writer = csv.DictWriter(csvfile, fieldnames=headers)
            
            writer.writeheader()
            for row in parsed_data['rows']:
                writer.writerow(row)
        
        return True
    
    except Exception as e:
        print("Error exporting to CSV: {0}".format(e))
        return False


# Example usage and testing
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 results_parser.py <path_to_creport.txt>")
        sys.exit(1)
    
    creport_file = sys.argv[1]
    
    print("Parsing {0}...".format(creport_file))
    results = parse_creport(creport_file)
    
    if results:
        print("\nMetadata:")
        for key, value in results['metadata'].items():
            print("  {0}: {1}".format(key, value))
        
        print("\nHeaders ({0} columns):".format(len(results['headers'])))
        print("  " + ", ".join(results['headers']))
        
        print("\nTotal corners: {0}".format(results['total_corners']))
        
        if results['rows']:
            print("\nFirst row sample:")
            for key, value in results['rows'][0].items():
                print("  {0}: {1}".format(key, value))
        
        # Analyze results
        print("\nAnalyzing results...")
        analysis = analyze_results(results)
        if analysis:
            print("  Pass: {0}".format(analysis['pass_count']))
            print("  Fail: {0}".format(analysis['fail_count']))
        
        # Export to CSV (optional)
        csv_path = creport_file.replace('.txt', '.csv')
        if export_to_csv(results, csv_path):
            print("\nExported to: {0}".format(csv_path))
    else:
        print("Failed to parse creport.txt")
        sys.exit(1)
